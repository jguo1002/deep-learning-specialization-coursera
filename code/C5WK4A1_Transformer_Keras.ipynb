{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690695963789,"user":{"displayName":"Jiajing Guo","userId":"11404290088358075388"},"user_tz":420},"id":"WdNPj2NAwdQV"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":19244,"status":"ok","timestamp":1690695983030,"user":{"displayName":"Jiajing Guo","userId":"11404290088358075388"},"user_tz":420},"id":"ixObuTNGweTl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"62519af9-880b-4d63-a820-7c2ca97b4935"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n","/gdrive/My Drive/Coursera/Coursera_DeepLearningSpecialization/course5_coding_assignments/W4A1\n"]}],"source":["# mount google drive. data and utils\n","from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My\\ Drive/Coursera/Coursera_DeepLearningSpecialization/course5_coding_assignments/W4A1/"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W6rQteoQT-p2","executionInfo":{"status":"ok","timestamp":1690696002862,"user_tz":420,"elapsed":19836,"user":{"displayName":"Jiajing Guo","userId":"11404290088358075388"}},"outputId":"9ba24d3d-0ecc-47ba-aa33-c3461ae085f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":15581,"status":"ok","timestamp":1690696018438,"user":{"displayName":"Jiajing Guo","userId":"11404290088358075388"},"user_tz":420},"id":"T0mnZ6JZhA0B"},"outputs":[],"source":["import tensorflow as tf\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization\n","from transformers import DistilBertTokenizerFast\n","from transformers import TFDistilBertForTokenClassification\n","from utils import create_padding_mask, create_look_ahead_mask, FullyConnected, positional_encoding\n","%load_ext autoreload\n","%autoreload 2\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1690696018439,"user":{"displayName":"Jiajing Guo","userId":"11404290088358075388"},"user_tz":420},"id":"d2rPbVuCqLdi"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"hh_WXe79lE4g"},"source":["- Encoder\n","- Decoder\n","- Transformer\n","\n","Resources:\n","- [TensorFlow - A Transformer Chatbot Tutorial with TensorFlow 2.0](https://medium.com/tensorflow/a-transformer-chatbot-tutorial-with-tensorflow-2-0-88bf59e66fe2)\n","- [ML Mastery - Implementing the Transformer Encoder from Scratch in TensorFlow and Keras](https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras/)\n","- [ML Mastery - Implementing the Transformer Decoder from Scratch in TensorFlow and Keras](https://machinelearningmastery.com/implementing-the-transformer-decoder-from-scratch-in-tensorflow-and-keras/)\n"]},{"cell_type":"markdown","source":["## Encoder"],"metadata":{"id":"TyvBYYk9cCAl"}},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":289,"status":"ok","timestamp":1690696018724,"user":{"displayName":"Jiajing Guo","userId":"11404290088358075388"},"user_tz":420},"id":"c5dvt0D-qLgo"},"outputs":[],"source":["class EncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self,\n","               embedding_dim,\n","               num_heads,\n","               fully_connected_dim,\n","               dropout_rate=0.1,\n","               layernorm_eps=1e-6,\n","               ):\n","    super(EncoderLayer, self).__init__()\n","    self.att = MultiHeadAttention(num_heads=num_heads,\n","                                  key_dim=embedding_dim,\n","                                  dropout=dropout_rate)\n","    self.ffn = tf.keras.Sequential([\n","        tf.keras.layers.Dense(fully_connected_dim, activation=\"relu\"),\n","        tf.keras.layers.Dense(embedding_dim)\n","    ])\n","\n","    self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n","    self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n","    self.dropout1 = Dropout(dropout_rate)\n","    self.dropout2 = Dropout(dropout_rate)\n","\n","  def call(self, x, training, mask):\n","    attn_output = self.att(x, x, x, mask)\n","    # attn_output = self.dropout1(attn_output, training = training)\n","    normed_output = self.layernorm1(x + attn_output)\n","    ffn_output = self.ffn(normed_output)\n","    ffn_output = self.dropout2(ffn_output, training = training)\n","    encoder_layer_out = self.layernorm2(normed_output + ffn_output)\n","    return encoder_layer_out\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690696018724,"user":{"displayName":"Jiajing Guo","userId":"11404290088358075388"},"user_tz":420},"id":"dFMQusg0vAYg"},"outputs":[],"source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self,\n","               num_layers,\n","               embedding_dim,\n","               num_heads,\n","               fully_connected_dim,\n","               input_vocab_size,\n","               maximum_position_encoding,\n","               dropout_rate=0.1,\n","               layernorm_eps=1e-6,\n","               ):\n","    super(Encoder, self).__init__()\n","    self.num_layers = num_layers\n","    self.embedding_dim = embedding_dim\n","    self.embedding = Embedding(input_vocab_size, embedding_dim)\n","    self.pos_encoding = positional_encoding(maximum_position_encoding,\n","                                            embedding_dim)\n","    self.enc_layers = [EncoderLayer(embedding_dim=embedding_dim,\n","                                    num_heads=num_heads,\n","                                    fully_connected_dim=fully_connected_dim,\n","                                    dropout_rate=dropout_rate,\n","                                    layernorm_eps=layernorm_eps)\n","                      for _ in range(num_layers)]\n","    self.dropout = Dropout(dropout_rate)\n","\n","  def call(self,\n","           inputs,\n","           training,\n","           mask):\n","    \"\"\"\n","    Args:\n","      - inputs:tf.Tensor (batch_size, input_seq_len, fully_connected_dim)\n","      - training:bool\n","      - mask:\n","    Returns:\n","      - outputs:tf.Tensor(batch_size, input_seq_len, fully_connected_dim)\n","    \"\"\"\n","\n","    seq_len = tf.shape(inputs)[1]\n","    # Pass input through the Embedding layer\n","    embeddings = self.embedding(inputs)\n","    # Scale embedding by multiplying it by the square root of the embedding dimension\n","    embeddings *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n","    # Add the position encoding to embedding\n","    pos_encoding_output = embeddings + self.pos_encoding[:, :seq_len, :]\n","    outputs = self.dropout(pos_encoding_output, training = training)\n","    for i in range(self.num_layers):\n","      outputs = self.enc_layers[i](outputs, training, mask)\n","    return outputs"]},{"cell_type":"markdown","source":["## Decoder"],"metadata":{"id":"v67XXR1mcE4f"}},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":241,"status":"ok","timestamp":1690696018960,"user":{"displayName":"Jiajing Guo","userId":"11404290088358075388"},"user_tz":420},"id":"ot4eYQ6CGmJA"},"outputs":[],"source":["class DecoderLayer(tf.keras.layers.Layer):\n","  def __init__(self,\n","               embedding_dim,\n","               num_heads,\n","               fully_connected_dim,\n","               dropout_rate=0.1,\n","               layernorm_eps=1e-6):\n","    super(DecoderLayer, self).__init__()\n","    self.att1 = MultiHeadAttention(num_heads=num_heads,\n","                                  key_dim=embedding_dim,\n","                                  dropout=dropout_rate)\n","    self.att2 = MultiHeadAttention(num_heads=num_heads,\n","                                  key_dim=embedding_dim,\n","                                  dropout=dropout_rate)\n","    self.ffn = tf.keras.Sequential([\n","        tf.keras.layers.Dense(fully_connected_dim, activation=\"relu\"),\n","        tf.keras.layers.Dense(embedding_dim)\n","    ])\n","    self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n","    self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n","    self.layernorm3 = LayerNormalization(epsilon=layernorm_eps)\n","    self.dropout1 = Dropout(dropout_rate)\n","    self.dropout2 = Dropout(dropout_rate)\n","    self.dropout3 = Dropout(dropout_rate)\n","\n","  def call(self,\n","           x,\n","           enc_output,\n","           training,\n","           look_ahead_mask,\n","           padding_mask):\n","    # att_output, att_weights_block\n","    attn_output1, attn_weights_block1 = self.att1(x, x, x, look_ahead_mask, return_attention_scores=True)\n","    attn_output1 = self.dropout1(attn_output1, training = training)\n","    normed_output1 = self.layernorm1(attn_output1 + x)\n","    attn_output2, attn_weights_block2 = self.att2(normed_output1, enc_output, enc_output, padding_mask, return_attention_scores=True)\n","    attn_output2 = self.dropout2(attn_output2, training = training)\n","    normed_output2 = self.layernorm2(normed_output1 + attn_output2)\n","    ffn_output = self.ffn(normed_output2)\n","    ffn_output = self.dropout3(ffn_output, training = training)\n","    normed_output3 = self.layernorm3(ffn_output + normed_output2)\n","    return normed_output3, attn_weights_block1, attn_weights_block2"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":188,"status":"ok","timestamp":1690696019146,"user":{"displayName":"Jiajing Guo","userId":"11404290088358075388"},"user_tz":420},"id":"BOleyZhGlVHW"},"outputs":[],"source":["class Decoder(tf.keras.layers.Layer):\n","  def __init__(self,\n","               num_layers,\n","               embedding_dim,\n","               num_heads,\n","               fully_connected_dim,\n","               target_vocab_size,\n","               maximum_position_encoding,\n","               dropout_rate=0.1,\n","               layernorm_eps=1e-6):\n","    super(Decoder, self).__init__()\n","    self.num_layers = num_layers\n","    self.embedding_dim = embedding_dim\n","    self.embedding = Embedding(target_vocab_size, embedding_dim)\n","    self.pos_encoding = positional_encoding(maximum_position_encoding, embedding_dim)\n","    self.dec_layers = [DecoderLayer(embedding_dim=embedding_dim,\n","                                    num_heads=num_heads,\n","                                    fully_connected_dim=fully_connected_dim,\n","                                    dropout_rate=dropout_rate,\n","                                    layernorm_eps=layernorm_eps)\n","                      for _ in range(num_layers)]\n","    self.dropout = Dropout(dropout_rate)\n","\n","  def call(self, x,\n","           encoding_output,\n","           training,\n","           look_ahead_mask,\n","           padding_mask):\n","    seq_len = tf.shape(x)[1]\n","    attention_weights = {}\n","\n","    embeddings = self.embedding(x)  # (batch_size, target_seq_len, fully_connected_dim)\n","    embeddings *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n","    pos_encoding_output = embeddings + self.pos_encoding[:, :seq_len, :]\n","    outputs = self.dropout(pos_encoding_output, training = training)\n","\n","    for i in range(self.num_layers):\n","      outputs, block1, block2 = self.dec_layers[i](outputs,\n","                                             encoding_output,\n","                                             training,\n","                                             look_ahead_mask,\n","                                             padding_mask)\n","      attention_weights[f\"decoder_layer{i+1}_block1_self_att\"] = block1\n","      attention_weights[f\"decoder_layer{i+1}_block2_decenc_att\"] = block2\n","\n","    return outputs, attention_weights\n"]},{"cell_type":"markdown","source":["## Transformer"],"metadata":{"id":"YaB0yphkcHXx"}},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":577,"status":"ok","timestamp":1690697625627,"user":{"displayName":"Jiajing Guo","userId":"11404290088358075388"},"user_tz":420},"id":"WCPSqfCNqLjc"},"outputs":[],"source":["class Transformer(tf.keras.Model):\n","  def __init__(self,\n","               num_layers,\n","               embedding_dim,\n","               num_heads,\n","               fully_connected_dim,\n","               input_vocab_size,\n","               target_vocab_size,\n","               max_positional_encoding_input,\n","               max_positional_encoding_target,\n","               dropout_rate = 0.1,\n","               layernorm_eps = 1e-6\n","               ):\n","      super(Transformer, self).__init__()\n","\n","      self.encoder = Encoder(num_layers=num_layers,\n","                               embedding_dim=embedding_dim,\n","                               num_heads=num_heads,\n","                               fully_connected_dim=fully_connected_dim,\n","                               input_vocab_size=input_vocab_size,\n","                               maximum_position_encoding=max_positional_encoding_input,\n","                               dropout_rate=dropout_rate,\n","                               layernorm_eps=layernorm_eps)\n","\n","      self.decoder = Decoder(num_layers=num_layers,\n","                              embedding_dim=embedding_dim,\n","                              num_heads=num_heads,\n","                              fully_connected_dim=fully_connected_dim,\n","                              target_vocab_size=target_vocab_size,\n","                              maximum_position_encoding=max_positional_encoding_target,\n","                              dropout_rate=dropout_rate,\n","                              layernorm_eps=layernorm_eps)\n","\n","      self.final_layer = Dense(target_vocab_size, activation='softmax')\n","\n","  def call(self,\n","           input_sentence,\n","           output_sentence,\n","           training: bool,\n","           encoding_padding_mask,\n","           look_ahead_mask,\n","           decoding_padding_mask):\n","    encoding_output = self.encoder(input_sentence, training, encoding_padding_mask)\n","    decoding_output, attention_weights = self.decoder(output_sentence,\n","                                                      encoding_output,\n","                                                      training,\n","                                                      look_ahead_mask,\n","                                                      decoding_padding_mask)\n","    final_output = self.final_layer(decoding_output)\n","    return final_output, attention_weights\n"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":1589,"status":"ok","timestamp":1690697780431,"user":{"displayName":"Jiajing Guo","userId":"11404290088358075388"},"user_tz":420},"id":"FB49NUEmqxUW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5bebe6e4-f3b9-4208-c55b-cd7c2f5e59fb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 5, 35), dtype=float32, numpy=\n","array([[[0.02122542, 0.03951558, 0.03045573, 0.01886736, 0.01792597,\n","         0.01548912, 0.0270176 , 0.01604852, 0.02647521, 0.05826805,\n","         0.01791989, 0.03370154, 0.03252686, 0.02306971, 0.04643841,\n","         0.01117709, 0.01843351, 0.03842779, 0.02485414, 0.02299673,\n","         0.04306052, 0.03768206, 0.02643834, 0.00933093, 0.01064945,\n","         0.047329  , 0.05123758, 0.01787438, 0.02077915, 0.03096425,\n","         0.05060191, 0.01340286, 0.05078856, 0.03535206, 0.01367469],\n","        [0.02821478, 0.0307212 , 0.02532112, 0.01353185, 0.01700304,\n","         0.01946055, 0.02366202, 0.01418533, 0.02688376, 0.04471272,\n","         0.01996359, 0.04141346, 0.02982285, 0.01805617, 0.03065509,\n","         0.01011677, 0.01725758, 0.04791638, 0.02592303, 0.0316627 ,\n","         0.03940037, 0.05005689, 0.03631482, 0.01068982, 0.01044376,\n","         0.05724731, 0.05910851, 0.01487506, 0.01838412, 0.03088713,\n","         0.04143605, 0.01502385, 0.05358619, 0.02730034, 0.01876171],\n","        [0.03659239, 0.02486032, 0.02143659, 0.01010043, 0.01881641,\n","         0.02730633, 0.02122909, 0.01483934, 0.02748556, 0.02955985,\n","         0.02390102, 0.04410316, 0.02999741, 0.01644109, 0.02006407,\n","         0.00944655, 0.01462577, 0.05543103, 0.03256131, 0.03733595,\n","         0.03512922, 0.05511167, 0.04727196, 0.01436184, 0.01122441,\n","         0.05926384, 0.05073575, 0.01345334, 0.01893535, 0.02782385,\n","         0.03036483, 0.01771861, 0.05292534, 0.01944623, 0.03010006],\n","        [0.04051212, 0.02245097, 0.0199872 , 0.00930406, 0.02007245,\n","         0.03190958, 0.02036574, 0.01564166, 0.02762384, 0.02404546,\n","         0.02612059, 0.04398031, 0.02968464, 0.01610634, 0.01660565,\n","         0.00973838, 0.01402568, 0.05658867, 0.03559936, 0.03954503,\n","         0.03254969, 0.05509645, 0.05159406, 0.0171198 , 0.01222794,\n","         0.05720044, 0.04533717, 0.01333003, 0.01950487, 0.0263513 ,\n","         0.02589021, 0.01961264, 0.05020568, 0.0168263 , 0.03724569],\n","        [0.03847146, 0.02298765, 0.02048803, 0.0097772 , 0.01843429,\n","         0.02829307, 0.02057756, 0.01448403, 0.02728596, 0.02768973,\n","         0.02434773, 0.04558049, 0.02838134, 0.01544179, 0.01816382,\n","         0.00981101, 0.015141  , 0.05593522, 0.03126142, 0.04038976,\n","         0.03343961, 0.05762207, 0.04941407, 0.0151125 , 0.01165235,\n","         0.05975645, 0.05222344, 0.01318621, 0.01819689, 0.02793328,\n","         0.02872604, 0.01855471, 0.05111664, 0.01863306, 0.03149013]]],\n","      dtype=float32)>"]},"metadata":{},"execution_count":21}],"source":["num_layers = 6\n","embedding_dim = 4\n","num_heads = 4\n","fully_connected_dim = 8\n","input_vocab_size = 30\n","target_vocab_size = 35\n","max_positional_encoding_input = 5\n","max_positional_encoding_target = 6\n","sentence_lang_a = np.array([[2, 1, 4, 3, 0]])\n","sentence_lang_b = np.array([[3, 2, 1, 0, 0]])\n","enc_padding_mask = create_padding_mask(sentence_lang_a)\n","dec_padding_mask = create_padding_mask(sentence_lang_b)\n","look_ahead_mask = create_look_ahead_mask(sentence_lang_a.shape[1])\n","\n","trans = Transformer(num_layers,\n","              embedding_dim,\n","              num_heads,\n","              fully_connected_dim,\n","              input_vocab_size,\n","              target_vocab_size,\n","              max_positional_encoding_input,\n","              max_positional_encoding_target)\n","\n","translation, weights = trans(\n","        sentence_lang_a,\n","        sentence_lang_b,\n","        False,\n","        enc_padding_mask,\n","        look_ahead_mask,\n","        dec_padding_mask\n","    )\n","\n","translation"]},{"cell_type":"code","source":[],"metadata":{"id":"yOhzb_Sfa3d6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-5nADOywWJrL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ce42kDcTUQod","executionInfo":{"status":"ok","timestamp":1690696019147,"user_tz":420,"elapsed":4,"user":{"displayName":"Jiajing Guo","userId":"11404290088358075388"}}},"execution_count":8,"outputs":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}